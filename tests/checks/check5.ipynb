{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f0823b",
   "metadata": {},
   "source": [
    "cut paste from roblinreg code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff63427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from src.magcal.characteristics import GnosticsCharacteristics\n",
    "\n",
    "class GnosticRobustRegression(GnosticsCharacteristics):\n",
    "    \"\"\"\n",
    "    A class to perform Gnostic Robust Regression based on the iterative formula\n",
    "    described in Equation 19.2 of the provided reference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, criterion=\"E1\", max_iter=100, tol=1e-6, degree=1, gc_history=False, verbose=False):\n",
    "            \"\"\"\n",
    "            Initialize the regression model.\n",
    "\n",
    "            Parameters:\n",
    "            - X: np.ndarray, shape (n_samples, n_features)\n",
    "                The input feature matrix.\n",
    "            - y: np.ndarray, shape (n_samples,)\n",
    "                The target vector.\n",
    "            - criterion: str\n",
    "                The gnostic criterion to use (e.g., \"Q1\", \"E1\", \"Q2\", etc.).\n",
    "            - max_iter: int\n",
    "                Maximum number of iterations for the regression.\n",
    "            - tol: float\n",
    "                Tolerance for convergence.\n",
    "            - degree: int\n",
    "                The degree of the polynomial regression. Default is 1 (linear regression).\n",
    "            - gc_history: bool\n",
    "                If True, records the history of gnostic characteristics (hi, hj, fi, fj).\n",
    "            - verbose: bool\n",
    "                If True, prints additional information during fitting.\n",
    "            \"\"\"\n",
    "            # data checking\n",
    "            if len(X.shape) != 2:\n",
    "                raise ValueError(\"X must be a 2D array.\")\n",
    "            if len(y.shape) != 1:\n",
    "                raise ValueError(\"y must be a 1D array.\")\n",
    "            if X.shape[0] != y.shape[0]:\n",
    "                raise ValueError(\"Number of samples in X and y must match.\")\n",
    "            \n",
    "            self.degree = degree\n",
    "            self.X = self._expand_features(X, degree)\n",
    "            self.y = y\n",
    "            self.criterion = criterion\n",
    "            self.max_iter = max_iter\n",
    "            self.tol = tol\n",
    "            self.gc_history = gc_history\n",
    "            self.weights = np.ones(self.X.shape[0])  # Initialize weights to 1\n",
    "            self.coefficients = None\n",
    "            self.history = {\"hi\": [], \"hj\": [], \"fi\": [], \"fj\": [], \"re\": []} if gc_history else None\n",
    "            self.verbose = verbose\n",
    "\n",
    "    def _compute_filtering_weight(self, residuals):\n",
    "        \"\"\"\n",
    "        Compute the filtering weights based on the selected gnostic criterion.\n",
    "\n",
    "        Parameters:\n",
    "        - residuals: np.ndarray, shape (n_samples,)\n",
    "            The residuals of the current iteration.\n",
    "\n",
    "        Returns:\n",
    "        - weights: np.ndarray, shape (n_samples,)\n",
    "            The updated filtering weights.\n",
    "        \"\"\"\n",
    "        # compute q and q1\n",
    "        eps_max = np.finfo(float).max\n",
    "        eps_min = np.finfo(float).eps\n",
    "        # y0 - the initial estimate\n",
    "        # Avoid division by zero or invalid values\n",
    "        y0 = np.dot(self.X, self.coefficients)\n",
    "        y0 = np.where(np.abs(y0) < eps_min, eps_min, y0)  # Replace near-zero values with eps_min\n",
    "\n",
    "        q = self.y / y0\n",
    "        q = np.where(np.abs(q) < eps_min, eps_min, q)  # Replace near-zero values with eps_min\n",
    "        q = np.where(np.abs(q) > eps_max, eps_max, q)  # Cap values exceeding eps_max\n",
    "\n",
    "        q1 = 1 / q\n",
    "        q1 = np.where(np.abs(q1) < eps_min, eps_min, q1)  # Replace near-zero values with eps_min\n",
    "        q1 = np.where(np.abs(q1) > eps_max, eps_max, q1)  # Cap values exceeding eps_max\n",
    "        \n",
    "        # gnostic characteristics\n",
    "        h_q = self._fi(q, q1)\n",
    "        f_q = 1 / (h_q + eps_min)  # Avoid division by zero\n",
    "        h_e = self._hi(q, q1)\n",
    "        f_e = 1 / (h_e + eps_min)\n",
    "\n",
    "        if self.criterion == \"Q1\":\n",
    "            return f_q\n",
    "        elif self.criterion == \"E1\":\n",
    "            return f_e ** 2\n",
    "        elif self.criterion == \"Q2\":\n",
    "            return np.ones_like(residuals)  # Constant weight for Q2\n",
    "        elif self.criterion == \"E2\":\n",
    "            return f_e\n",
    "        elif self.criterion == \"Q3\":\n",
    "            return 1 / np.sqrt(f_q)\n",
    "        elif self.criterion == \"E3\":\n",
    "            return np.sqrt(f_e)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown criterion: {self.criterion}\")\n",
    "\n",
    "    def _update_weights(self, residuals):\n",
    "        \"\"\"\n",
    "        Update the weights based on the residuals and the filtering function.\n",
    "\n",
    "        Parameters:\n",
    "        - residuals: np.ndarray, shape (n_samples,)\n",
    "            The residuals of the current iteration.\n",
    "        \"\"\"\n",
    "        self.weights = self._compute_filtering_weight(residuals)\n",
    "    \n",
    "    def _expand_features(self, X, degree):\n",
    "        \"\"\"\n",
    "        Expand the input feature matrix to include polynomial terms up to the specified degree.\n",
    "\n",
    "        Parameters:\n",
    "        - X: np.ndarray, shape (n_samples, n_features)\n",
    "            The input feature matrix.\n",
    "        - degree: int\n",
    "            The degree of the polynomial regression.\n",
    "\n",
    "        Returns:\n",
    "        - X_expanded: np.ndarray, shape (n_samples, n_expanded_features)\n",
    "            The expanded feature matrix with polynomial terms.\n",
    "        \"\"\"\n",
    "        X_expanded = X\n",
    "        for d in range(2, degree + 1):\n",
    "            X_expanded = np.hstack([X_expanded, X ** d])\n",
    "        return X_expanded\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Fit the regression model using the iterative weighted least squares approach.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = self.X.shape\n",
    "        X_weighted = self.X.copy()\n",
    "        y_weighted = self.y.copy()\n",
    "\n",
    "        # Initialize coefficients\n",
    "        self.coefficients = np.zeros(n_features)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Compute residuals\n",
    "            residuals = self.y - np.dot(self.X, self.coefficients)\n",
    "\n",
    "            # Update weights\n",
    "            self._update_weights(residuals)\n",
    "\n",
    "            # Apply weights to the design matrix and target vector\n",
    "            W = np.sqrt(self.weights)[:, np.newaxis]\n",
    "            X_weighted = W * self.X\n",
    "            y_weighted = W.flatten() * self.y\n",
    "\n",
    "            # Check for NaNs or infinities\n",
    "            if not np.isfinite(X_weighted).all() or not np.isfinite(y_weighted).all():\n",
    "                raise ValueError(\"NaN or infinity detected in weighted matrices. Check your data or weights.\")\n",
    "\n",
    "            # Solve the weighted least squares problem\n",
    "            new_coefficients = np.linalg.lstsq(X_weighted, y_weighted, rcond=None)[0]\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(new_coefficients - self.coefficients) < self.tol:\n",
    "                if self.verbose:\n",
    "                    print(f\"Converged in {iteration + 1} iterations.\")\n",
    "                break\n",
    "\n",
    "            self.coefficients = new_coefficients\n",
    "        else:\n",
    "            print(\"Maximum iterations reached without convergence.\")\n",
    "\n",
    "    # Modify predict method\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for the given input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: np.ndarray, shape (n_samples, n_features)\n",
    "            The input feature matrix.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: np.ndarray, shape (n_samples,)\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        X_expanded = self._expand_features(X, self.degree)\n",
    "        return np.dot(X_expanded, self.coefficients)\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Get the final weights after fitting the model.\n",
    "\n",
    "        Returns:\n",
    "        - weights: np.ndarray, shape (n_samples,)\n",
    "            The final weights.\n",
    "        \"\"\"\n",
    "        return self.weights\n",
    "\n",
    "    def get_coefficients(self):\n",
    "        \"\"\"\n",
    "        Get the fitted coefficients of the model.\n",
    "\n",
    "        Returns:\n",
    "        - coefficients: np.ndarray, shape (n_features,)\n",
    "            The fitted coefficients.\n",
    "        \"\"\"\n",
    "        return self.coefficients\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
