{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab721791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Get the absolute path to the project root\n",
    "# notebook_path = Path('/Users/nirmal/Documents/np_research/ManGo_code/ManGo')\n",
    "# project_root = str(notebook_path)\n",
    "\n",
    "# # Add the project root to Python path if it's not already there\n",
    "# if project_root not in sys.path:\n",
    "#     sys.path.insert(0, project_root)\n",
    "\n",
    "# # Now you can import your module\n",
    "# from machinegnostics.magcal import gmedian\n",
    "\n",
    "# # Verify the import worked\n",
    "# print(f\"Project root: {project_root}\")\n",
    "# print(f\"Available at: {gmedian.__module__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data\n",
    "import numpy as np\n",
    "data = np.array([1.2, 2.3, 3.4, 4.5, 5.6])\n",
    "\n",
    "# Using function directly\n",
    "g_median_quant = gmedian(data, case='j')\n",
    "print(f\"i G-median: {g_median_quant:.4f}\")\n",
    "\n",
    "g_median_esti = gmedian(data, case='i')\n",
    "print(f\"j G-median: {g_median_esti:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbce12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'median {np.median(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe1be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mean {np.mean(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinegnostics.magcal.gmodulus import gmodulus\n",
    "from machinegnostics.magcal.gvar import gvariance\n",
    "from machinegnostics.magcal.gacov import gautocovariance\n",
    "from machinegnostics.magcal import gcorrelation, gvariance, gmodulus, gautocovariance, gcovariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 1, 1, 1.3, 1.1, 1, 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1753b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using function directly\n",
    "g_median_quant = gmedian(data, case='j')\n",
    "print(f\"i G-median: {g_median_quant:.4f}\")\n",
    "\n",
    "g_median_esti = gmedian(data, case='i')\n",
    "print(f\"j G-median: {g_median_esti:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5808eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mi = gmodulus(data, case='i')\n",
    "Mj = gmodulus(data, case='j')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27bcbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mi {Mi}, Mj {Mj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b612f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gvar_i = gvariance(data, 'i')\n",
    "gvar_j = gvariance(data, 'j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Gvar i:{gvar_i}, Gvar j:{gvar_j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa185a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of gvariance\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "gvar_i = gvariance(data, 'i')\n",
    "gvar_j = gvariance(data, 'j')\n",
    "print(f'Gvar i:{gvar_i}, Gvar j:{gvar_j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b08d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of gcorrelation\n",
    "data1 = np.array([1, 2, 3, 4, 5])\n",
    "data2 = np.array([5, 4, 3, 2, 1])\n",
    "gcor_i = gcorrelation(data1, data2, 'i')\n",
    "gcor_j = gcorrelation(data1, data2, 'j')\n",
    "print(f'Gcorrelation i:{gcor_i}, Gcorrelation j:{gcor_j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of gautocovariance\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "gautocov_i = gautocovariance(data, 'i')\n",
    "gautocov_j = gautocovariance(data, 'j')\n",
    "print(f'Gautocovariance i:{gautocov_i}, Gautocovariance j:{gautocov_j}')\n",
    "# example of gcovariance\n",
    "data1 = np.array([1, 2, 3, 4, 5])\n",
    "data2 = np.array([5, 4, 3, 2, 1])\n",
    "gcov_i = gcovariance(data1, data2, 'i')\n",
    "gcov_j = gcovariance(data1, data2, 'j')\n",
    "print(f'Gcovariance i:{gcov_i}, Gcovariance j:{gcov_j}')\n",
    "# example of gmodulus\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "gmod_i = gmodulus(data, 'i')\n",
    "gmod_j = gmodulus(data, 'j')\n",
    "print(f'Gmodulus i:{gmod_i}, Gmodulus j:{gmod_j}')\n",
    "# example of gmedian\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "gmed_i = gmedian(data, 'i')\n",
    "gmed_j = gmedian(data, 'j')\n",
    "print(f'Gmedian i:{gmed_i}, Gmedian j:{gmed_j}')\n",
    "# example of gcorrelation\n",
    "data1 = np.array([1, 2, 3, 4, 5])\n",
    "data2 = np.array([5, 4, 3, 2, 1])\n",
    "gcor_i = gcorrelation(data1, data2, 'i')\n",
    "gcor_j = gcorrelation(data1, data2, 'j')\n",
    "print(f'Gcorrelation i:{gcor_i}, Gcorrelation j:{gcor_j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([-100, -2, 3, 4, 5])\n",
    "\n",
    "# gnosticm median by np.median\n",
    "statistical_median = np.median(data)\n",
    "print(f\"Statistical median: {statistical_median}\")\n",
    "# gnosticm median by gmedian\n",
    "gmedian_i = gmedian(data, 'i')\n",
    "print(f\"Gnostic median (i): {gmedian_i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83621e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import robr2, gmmfe, divI, evalMet, hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "y = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_fit = np.array([0.9, 2.1, 2.9, 4.2, 4.8])\n",
    "weights = np.array([1, 1, 1, 1, 1])\n",
    "\n",
    "# Calculate the robust R-squared value\n",
    "robust_r2 = robr2(y, y_fit, weights)\n",
    "print(f\"Robust R-squared: {robust_r2:.4f}\")\n",
    "\n",
    "# Calculate the GMMFE\n",
    "gmmfe_value = gmmfe(y, y_fit)\n",
    "print(f\"GMMFE: {gmmfe_value:.4f}\")\n",
    "\n",
    "# Calculate the divergence\n",
    "divergence_value = divI(y, y_fit)\n",
    "print(f\"Divergence: {divergence_value:.4f}\")\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "metrics = evalMet(y, y_fit, weights)\n",
    "print(f\"Evaluation Metrics: {metrics}\")\n",
    "\n",
    "# hc\n",
    "hc_value = hc(y, y_fit)\n",
    "print(f\"hc: {hc_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a270d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Simulate more complex data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "x = np.linspace(0, 10, 20)\n",
    "y = 2.5 * np.sin(x) + 0.5 * x + np.random.normal(scale=.2, size=len(x))  # Non-linear data with noise\n",
    "# adding specific outlier \n",
    "y[5] = 0.55  # Adding an outlier\n",
    "y[7] = 7.5  # Adding another outlier\n",
    "y[13] = 12.9 # Adding another outlier\n",
    "# y[5] = 11.5 # Adding another outlier\n",
    "y_fit = 2.5 * np.sin(x) + 0.5 * x  # Ideal fit (without noise)\n",
    "weights = np.random.uniform(0.8, 1.2, size=len(x))  # Random weights\n",
    "\n",
    "# Calculate the robust R-squared value\n",
    "robust_r2 = robr2(y, y_fit, weights)\n",
    "print(f\"Robust R-squared: {robust_r2:.4f}\")\n",
    "\n",
    "# Calculate the GMMFE\n",
    "gmmfe_value = gmmfe(y, y_fit)\n",
    "print(f\"GMMFE: {gmmfe_value:.4f}\")\n",
    "\n",
    "# Calculate the divergence\n",
    "divergence_value = divI(y, y_fit)\n",
    "print(f\"Divergence: {divergence_value:.4f}\")\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "metrics = evalMet(y, y_fit, weights)\n",
    "print(f\"Evaluation Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb48761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import RobustRegressor\n",
    "\n",
    "mgr = RobustRegressor(degree=6)\n",
    "\n",
    "mgr.fit(x, y)\n",
    "\n",
    "y_pred = mgr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a808acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sk learn polinomial regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Create a polynomial regression model\n",
    "degree = 6\n",
    "polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "polyreg.fit(x.reshape(-1, 1), y)\n",
    "y_pred_sklearn = polyreg.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d593f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn's R-squared value\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"R-squared - MG: {r2:.4f}\")\n",
    "\n",
    "# sklearn's R-squared value for polynomial regression\n",
    "r2_sklearn = r2_score(y, y_pred_sklearn)\n",
    "print(f\"R-squared - sklearn: {r2_sklearn:.4f}\")\n",
    "\n",
    "# robust regressor evaluation\n",
    "robust_r2 = robr2(y, y_pred, mgr.weights)\n",
    "print(f\"Robust R-squared: {robust_r2:.4f}\")\n",
    "# Calculate the GMMFE\n",
    "gmmfe_value = gmmfe(y, y_pred)\n",
    "print(f\"GMMFE: {gmmfe_value:.4f}\")\n",
    "# Calculate the divergence\n",
    "divergence_value = divI(y, y_pred)\n",
    "print(f\"Divergence: {divergence_value:.4f}\")\n",
    "# Calculate the evaluation metrics\n",
    "metrics = evalMet(y, y_pred, mgr.weights)\n",
    "print(f\"Evaluation Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ed1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot y and y_fit with error bars\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, label='Observed Data', color='blue', alpha=0.6)\n",
    "plt.plot(x, y_fit, label='True Line Function', color='red', linewidth=2)\n",
    "# plot y_pred from robust regressor\n",
    "plt.plot(x, y_pred, label='Mango Fit', color='green', linestyle='--', linewidth=2)\n",
    "# plot y_pred from sklearn\n",
    "plt.plot(x, y_pred_sklearn, label='Sklearn Fit', color='orange', linestyle='--', linewidth=2)\n",
    "plt.errorbar(x, y, yerr=0.5, fmt='o', color='gray', alpha=0.5, label='Error Bars')\n",
    "plt.title('Observed Data vs Fitted Line')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416e759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3da303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from machinegnostics.metrics import gcorrelation\n",
    "import numpy as np\n",
    "\n",
    "# Example 1: Estimation case (should be close to 1 for linear relationship)\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y = np.array([0.9, 2.1, 2.9, 4.2, 4.8])\n",
    "gcor_i = gcorrelation(x, y)\n",
    "print(f\"Estimation correlation (case='i'): {gcor_i}\")\n",
    "\n",
    "# Example 2: Quantification case (should be positive for strong relationship)\n",
    "a = np.array([10.1, 10.3, 9.8, 10.2, 10.0])\n",
    "b = np.array([5.1, 5.2, 4.9, 5.3, 5.0])\n",
    "gcor_j = gcorrelation(a, b)\n",
    "print(f\"Quantification correlation (case='j'): {gcor_j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156dafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with other correlation\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "# Pearson correlation\n",
    "pearson_corr, _ = pearsonr(x, y)\n",
    "print(f\"Pearson correlation: {pearson_corr}\")\n",
    "\n",
    "# for and b\n",
    "pearson_corr, _ = pearsonr(a, b)\n",
    "print(f\"Pearson correlation: {pearson_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from machinegnostics.metrics import gcorrelation\n",
    "# from machinegnostics.magcal.gcor import gcorrelation\n",
    "\n",
    "# Example 2: Multiple pairs (2D arrays)\n",
    "X = np.array([\n",
    "    [1.0, 2.0, 3.0, 4.0, 5.0],   # 'a'\n",
    "    [2.0, 3.0, 4.0, 5.0, 6.0]    # 'b'\n",
    "])\n",
    "Y = np.array([\n",
    "    [0.9, 2.1, 2.9, 4.2, 4.8],   # 'j'\n",
    "    [1.8, 3.2, 4.1, 5.1, 6.2]    # 'k'\n",
    "])\n",
    "\n",
    "# Assign row/column names\n",
    "x_names = ['a', 'b']\n",
    "y_names = ['j', 'k']\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = gcorrelation(X, Y)\n",
    "\n",
    "# Convert to DataFrame for labeled output\n",
    "df_corr = pd.DataFrame(corr_matrix, index=x_names, columns=y_names)\n",
    "print(corr_matrix)\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fea8030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ManGo Metrics (manual prediction) ===\n",
      "Accuracy:  0.70\n",
      "Precision: 0.75\n",
      "Recall:    0.60\n",
      "F1 Score:  0.67\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [2 3]]\n",
      "Classification Report:\n",
      "Class           Precision    Recall  F1-score   Support\n",
      "========================================================\n",
      "0                    0.67      0.80      0.73         5\n",
      "1                    0.75      0.60      0.67         5\n",
      "========================================================\n",
      "Avg/Total            0.71      0.70      0.70        10\n",
      "\n",
      "\n",
      "=== scikit-learn Metrics (manual prediction) ===\n",
      "Accuracy:  0.70\n",
      "Precision: 0.75\n",
      "Recall:    0.60\n",
      "F1 Score:  0.67\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [2 3]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73         5\n",
      "           1       0.75      0.60      0.67         5\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.71      0.70      0.70        10\n",
      "weighted avg       0.71      0.70      0.70        10\n",
      "\n",
      "\n",
      "=== ManGo Metrics (LogisticRegressor) ===\n",
      "Accuracy:  0.80\n",
      "Precision: 0.80\n",
      "Recall:    0.80\n",
      "F1 Score:  0.80\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [1 4]]\n",
      "Classification Report:\n",
      "Class           Precision    Recall  F1-score   Support\n",
      "========================================================\n",
      "0                    0.80      0.80      0.80         5\n",
      "1                    0.80      0.80      0.80         5\n",
      "========================================================\n",
      "Avg/Total            0.80      0.80      0.80        10\n",
      "\n",
      "\n",
      "=== scikit-learn Metrics (LogisticRegressor) ===\n",
      "Accuracy:  0.80\n",
      "Precision: 0.80\n",
      "Recall:    0.80\n",
      "F1 Score:  0.80\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [1 4]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80         5\n",
      "           1       0.80      0.80      0.80         5\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.80      0.80      0.80        10\n",
      "weighted avg       0.80      0.80      0.80        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Classification metrics and report comparison\n",
    "\n",
    "import numpy as np\n",
    "from machinegnostics.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "from machinegnostics.models import LogisticRegressor\n",
    "\n",
    "# Simulate a simple binary classification dataset\n",
    "y_true = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 0])\n",
    "y_pred = np.array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "print(\"=== ManGo Metrics (manual prediction) ===\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.2f}\")\n",
    "print(f\"Precision: {precision_score(y_true, y_pred, average='binary'):.2f}\")\n",
    "print(f\"Recall:    {recall_score(y_true, y_pred, average='binary'):.2f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_true, y_pred, average='binary'):.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Compare with scikit-learn\n",
    "from sklearn.metrics import accuracy_score as sk_acc, precision_score as sk_prec, recall_score as sk_rec, f1_score as sk_f1, confusion_matrix as sk_cm, classification_report as sk_report\n",
    "\n",
    "print(\"\\n=== scikit-learn Metrics (manual prediction) ===\")\n",
    "print(f\"Accuracy:  {sk_acc(y_true, y_pred):.2f}\")\n",
    "print(f\"Precision: {sk_prec(y_true, y_pred):.2f}\")\n",
    "print(f\"Recall:    {sk_rec(y_true, y_pred):.2f}\")\n",
    "print(f\"F1 Score:  {sk_f1(y_true, y_pred):.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(sk_cm(y_true, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(sk_report(y_true, y_pred, digits=2))\n",
    "\n",
    "# Example with LogisticRegressor (fit and predict)\n",
    "X = np.array([\n",
    "    [0.1, 1.2],\n",
    "    [1.1, 0.9],\n",
    "    [0.2, 1.0],\n",
    "    [1.0, 1.1],\n",
    "    [0.9, 0.2],\n",
    "    [1.2, 0.1],\n",
    "    [0.3, 1.3],\n",
    "    [1.3, 0.3],\n",
    "    [0.4, 1.4],\n",
    "    [1.4, 0.4]\n",
    "])\n",
    "y = np.array([0, 1, 0, 1, 1, 0, 0, 1, 0, 1])\n",
    "\n",
    "model = LogisticRegressor()\n",
    "model.fit(X, y)\n",
    "y_pred_model = model.predict(X)\n",
    "\n",
    "print(\"\\n=== ManGo Metrics (LogisticRegressor) ===\")\n",
    "print(f\"Accuracy:  {accuracy_score(y, y_pred_model):.2f}\")\n",
    "print(f\"Precision: {precision_score(y, y_pred_model, average='binary'):.2f}\")\n",
    "print(f\"Recall:    {recall_score(y, y_pred_model, average='binary'):.2f}\")\n",
    "print(f\"F1 Score:  {f1_score(y, y_pred_model, average='binary'):.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y, y_pred_model))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y, y_pred_model))\n",
    "\n",
    "print(\"\\n=== scikit-learn Metrics (LogisticRegressor) ===\")\n",
    "print(f\"Accuracy:  {sk_acc(y, y_pred_model):.2f}\")\n",
    "print(f\"Precision: {sk_prec(y, y_pred_model):.2f}\")\n",
    "print(f\"Recall:    {sk_rec(y, y_pred_model):.2f}\")\n",
    "print(f\"F1 Score:  {sk_f1(y, y_pred_model):.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(sk_cm(y, y_pred_model))\n",
    "print(\"Classification Report:\")\n",
    "print(sk_report(y, y_pred_model, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee4a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn Accuracy: 0.7\n",
      "sklearn Precision: 0.75\n",
      "sklearn Recall: 0.6\n",
      "sklearn F1: 0.6666666666666666\n",
      "sklearn Confusion Matrix:\n",
      " [[4 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "print(\"sklearn Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"sklearn Precision:\", precision_score(y_true, y_pred))\n",
    "print(\"sklearn Recall:\", recall_score(y_true, y_pred))\n",
    "print(\"sklearn F1:\", f1_score(y_true, y_pred))\n",
    "print(\"sklearn Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81984d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
